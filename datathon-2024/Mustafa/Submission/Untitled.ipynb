{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427cdab9-b474-4be9-9604-2a6ceff82437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, mean_squared_error\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('cleanedData.csv')\n",
    "\n",
    "# Drop rows with missing target values\n",
    "df = df.dropna(subset=['degerlendirme_puani'])\n",
    "\n",
    "# Extract relevant columns for the classification model\n",
    "cat_features = [\n",
    " 'cinsiyet',\n",
    " 'dogum_yeri',\n",
    " 'ikametgah_sehri',\n",
    " 'universite_adi',\n",
    " 'universite_turu',\n",
    " 'burslu_ise_burs_yuzdesi',\n",
    " 'burs_aliyor_mu?',\n",
    " 'universite_kacinci_sinif',\n",
    " 'universite_not_ortalamasi',\n",
    " 'daha_once_baska_bir_universiteden_mezun_olmus',\n",
    " 'lise_adi',\n",
    " 'lise_adi_diger',\n",
    " 'lise_sehir',\n",
    " 'lise_turu',\n",
    " 'lise_mezuniyet_notu',\n",
    " 'baska_bir_kurumdan_burs_aliyor_mu?',\n",
    " 'burs_aldigi_baska_kurum',\n",
    " 'baska_kurumdan_aldigi_burs_miktari',\n",
    " 'anne_egitim_durumu',\n",
    " 'anne_calisma_durumu',\n",
    " 'baba_egitim_durumu',\n",
    " 'baba_calisma_durumu',\n",
    " 'kardes_sayisi',\n",
    " 'girisimcilik_kulupleri_tarzi_bir_kulube_uye_misiniz?',\n",
    " 'profesyonel_bir_spor_daliyla_mesgul_musunuz?',\n",
    " 'aktif_olarak_bir_stk_uyesi_misiniz?',\n",
    " 'stk_projesine_katildiniz_mi?',\n",
    " 'girisimcilikle_ilgili_deneyiminiz_var_mi?',\n",
    " 'ingilizce_biliyor_musunuz?',\n",
    " 'ingilizce_seviyeniz?',\n",
    " 'daha_onceden_mezun_olunduysa_mezun_olunan_universite',\n",
    " 'anne_sektor_encoded',\n",
    " 'baba_sektor_encoded',\n",
    " 'anne_unknown',\n",
    " 'anne_diger',\n",
    " 'anne_kamu',\n",
    " 'anne_ozel_sektor',\n",
    " 'baba_unknown',\n",
    " 'baba_diger',\n",
    " 'baba_kamu',\n",
    " 'baba_ozel_sektor',\n",
    " 'age'\n",
    "]\n",
    "\n",
    "text_columns = [\n",
    "    'girisimcilikle_ilgili_deneyiminizi_aciklayabilir_misiniz?', \n",
    "    'bolum', \n",
    "    'lise_bolumu', \n",
    "    'lise_bolum_diger',\n",
    "    'uye_oldugunuz_kulubun_ismi',\n",
    "    'spor_dalindaki_rolunuz_nedir?',\n",
    "    \"hangi_stk_nin_uyesisiniz?\",\n",
    "     ]\n",
    "target_column = 'degerlendirme_puani'\n",
    "\n",
    "# Handle missing values in text columns\n",
    "df[text_columns] = df[text_columns].fillna('')\n",
    "\n",
    "# Combine text columns into one for processing\n",
    "df['combined_text'] = df[text_columns].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# Tokenize and preprocess text data for Word2Vec\n",
    "def preprocess_text(text):\n",
    "    return simple_preprocess(text, deacc=True)\n",
    "\n",
    "# Check if the Word2Vec model already exists\n",
    "import os\n",
    "if os.path.exists('word2vec_model.model'):\n",
    "    # Load the previously saved Word2Vec model\n",
    "    word2vec_model = Word2Vec.load('word2vec_model.model')\n",
    "else:\n",
    "    # Prepare Word2Vec model\n",
    "    sentences = [preprocess_text(text) for text in df['combined_text']]\n",
    "    word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    # Save the Word2Vec model\n",
    "    word2vec_model.save('word2vec_model.model')\n",
    "\n",
    "# Create document vectors by averaging word vectors\n",
    "def vectorize_text(text):\n",
    "    tokens = preprocess_text(text)\n",
    "    vectors = [word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "df['text_vector'] = df['combined_text'].apply(vectorize_text)\n",
    "word2vec_features = np.array(df['text_vector'].tolist())\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "text_data = df['combined_text']\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Combine Word2Vec and TF-IDF features\n",
    "combined_features = hstack([tfidf_vectors, word2vec_features])\n",
    "\n",
    "# Debugging step: Verify columns in df and missing columns in cat_features\n",
    "missing_cols = [col for col in cat_features if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(\"Warning: The following columns are missing from the DataFrame:\", missing_cols)\n",
    "\n",
    "# Prepare data for classification model\n",
    "try:\n",
    "    X_cat = df[cat_features]\n",
    "except KeyError as e:\n",
    "    print(\"Error: One or more columns are missing:\", e)\n",
    "    print(\"Available columns:\", df.columns)\n",
    "    raise\n",
    "\n",
    "y = df[target_column]\n",
    "\n",
    "# ColumnTransformer for encoding categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Classification pipeline\n",
    "clf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Split data for classification model\n",
    "X_cat_train, X_cat_test, y_train, y_test = train_test_split(X_cat, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train classification model\n",
    "clf_pipeline.fit(X_cat_train, y_train)\n",
    "\n",
    "# Evaluate classification model\n",
    "y_pred_cat = clf_pipeline.predict(X_cat_test)\n",
    "print(\"Classification Model Accuracy:\", accuracy_score(y_test, y_pred_cat))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_cat))\n",
    "\n",
    "# Prepare data for regression model\n",
    "# Debugging step: Print shapes and types\n",
    "print(\"Shape of X_cat:\", X_cat.shape)\n",
    "print(\"Shape of combined_features:\", combined_features.shape)\n",
    "print(\"Type of X_cat:\", type(X_cat))\n",
    "print(\"Type of combined_features:\", type(combined_features))\n",
    "print(\"Dtype of X_cat:\", X_cat.dtypes)\n",
    "print(\"Dtype of combined_features:\", combined_features.dtype)\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = X_cat.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X_cat.select_dtypes(include=['object', 'bool']).columns\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=True), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Fit and transform the data\n",
    "X_cat_preprocessed = preprocessor.fit_transform(X_cat)\n",
    "\n",
    "# Debugging step: Print shapes after preprocessing\n",
    "print(\"Shape of X_cat_preprocessed:\", X_cat_preprocessed.shape)\n",
    "print(\"Shape of combined_features:\", combined_features.shape)\n",
    "\n",
    "# Combine features\n",
    "X_combined = hstack([X_cat_preprocessed, combined_features])\n",
    "\n",
    "# Debugging step: Print final combined shape\n",
    "print(\"Shape of X_combined:\", X_combined.shape)\n",
    "\n",
    "# Split data for regression model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modify CatBoost hyperparameters\n",
    "catboost_model = CatBoostRegressor(\n",
    "    iterations=2000,  # Increased iterations\n",
    "    learning_rate=0.05,  # Lowered learning rate\n",
    "    depth=8,  # Increased depth\n",
    "    l2_leaf_reg=3,  # Added L2 regularization\n",
    "    loss_function='RMSE',\n",
    "    eval_metric='RMSE',\n",
    "    random_seed=42,\n",
    "    verbose=100,\n",
    "    early_stopping_rounds=100  # Increased early stopping rounds\n",
    ")\n",
    "\n",
    "# Add feature selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "selector = SelectFromModel(estimator=CatBoostRegressor(iterations=100), threshold='median')\n",
    "X_selected = selector.fit_transform(X_combined, y)\n",
    "\n",
    "# Split data using selected features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model with selected features\n",
    "catboost_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=(X_test, y_test),\n",
    "    use_best_model=True,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(catboost_model, X_combined, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f'Cross-validation RMSE: {(-cv_scores.mean())**0.5:.4f} (+/- {cv_scores.std() * 2:.4f})')\n",
    "\n",
    "# Predict and evaluate regression model\n",
    "y_pred_reg = catboost_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred_reg)\n",
    "rmse = mse ** 0.5\n",
    "print(f'Regression Model RMSE: {rmse:.4f}')\n",
    "\n",
    "# Print actual vs predicted values for regression\n",
    "print(\"\\nSample Actual vs. Predicted Values:\")\n",
    "for actual, predicted in zip(y_test[:10], y_pred_reg[:10]):\n",
    "    print(f\"Actual: {actual:.2f}, Predicted: {predicted:.2f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = catboost_model.feature_importances_\n",
    "feature_names = preprocessor.get_feature_names_out().tolist() + [f'text_feature_{i}' for i in range(combined_features.shape[1])]\n",
    "for name, importance in sorted(zip(feature_names, feature_importance), key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print(f\"Feature: {name}, Importance: {importance:.4f}\")\n",
    "\n",
    "# Save the CatBoost model\n",
    "catboost_model.save_model('catboost_model.cbm')\n",
    "\n",
    "# Load the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63b1a4d-5df7-4f95-a140-b0c96c72218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('cleanedTest.csv')\n",
    "\n",
    "# Preprocess test data\n",
    "test_df[text_columns] = test_df[text_columns].fillna('')\n",
    "test_df['combined_text'] = test_df[text_columns].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# Create document vectors for test data\n",
    "test_df['text_vector'] = test_df['combined_text'].apply(vectorize_text)\n",
    "test_word2vec_features = np.array(test_df['text_vector'].tolist())\n",
    "\n",
    "# TF-IDF Vectorization for test data\n",
    "test_tfidf_vectors = tfidf_vectorizer.transform(test_df['combined_text'])\n",
    "\n",
    "# Combine Word2Vec and TF-IDF features for test data\n",
    "test_combined_features = hstack([test_tfidf_vectors, test_word2vec_features])\n",
    "\n",
    "# Prepare test data for the model\n",
    "X_test_cat = test_df[cat_features]\n",
    "\n",
    "# Preprocess test data\n",
    "X_test_cat_preprocessed = preprocessor.transform(X_test_cat)\n",
    "\n",
    "# Combine features for test data\n",
    "X_test_combined = hstack([X_test_cat_preprocessed, test_combined_features])\n",
    "\n",
    "# Apply feature selection\n",
    "X_test_selected = selector.transform(X_test_combined)\n",
    "\n",
    "# Load the saved CatBoost model\n",
    "loaded_model = CatBoostRegressor()\n",
    "loaded_model.load_model('catboost_model.cbm')\n",
    "\n",
    "# Make predictions\n",
    "test_predictions = loaded_model.predict(X_test_selected)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'Degerlendirme Puani': test_predictions\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Predictions have been saved to 'submission.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ff4f1a-4090-4a28-8a83-8d72038276e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
