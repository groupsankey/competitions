{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427cdab9-b474-4be9-9604-2a6ceff82437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Model Accuracy: 0.22910787931408727\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.00      0.00      0.00         4\n",
      "         2.0       0.00      0.00      0.00         2\n",
      "         3.0       0.57      1.00      0.73         4\n",
      "         4.0       0.50      0.36      0.42        11\n",
      "         5.0       0.32      0.65      0.43        95\n",
      "         6.0       0.45      0.84      0.58        86\n",
      "         7.0       0.38      0.82      0.52       237\n",
      "         8.0       0.49      0.57      0.53       154\n",
      "         9.0       0.44      0.66      0.53       138\n",
      "        10.0       0.35      0.37      0.36       134\n",
      "        11.0       0.28      0.29      0.28       119\n",
      "        12.0       0.44      0.47      0.46       274\n",
      "        13.0       0.21      0.21      0.21       156\n",
      "        14.0       0.37      0.37      0.37       236\n",
      "        15.0       0.23      0.27      0.25       177\n",
      "        16.0       0.38      0.39      0.39       188\n",
      "        17.0       0.31      0.24      0.27       137\n",
      "        18.0       0.29      0.33      0.31       229\n",
      "        19.0       0.28      0.31      0.29       190\n",
      "        20.0       0.20      0.16      0.18       259\n",
      "        21.0       0.20      0.19      0.20       173\n",
      "        22.0       0.27      0.36      0.31       291\n",
      "        23.0       0.08      0.07      0.07       167\n",
      "        24.0       0.35      0.43      0.39       251\n",
      "        25.0       0.12      0.07      0.09       148\n",
      "        26.0       0.28      0.35      0.31       242\n",
      "        27.0       0.19      0.15      0.17       204\n",
      "        28.0       0.18      0.18      0.18       291\n",
      "        29.0       0.18      0.17      0.18       185\n",
      "        30.0       0.13      0.11      0.12       234\n",
      "        31.0       0.14      0.12      0.13       153\n",
      "        32.0       0.15      0.14      0.14       222\n",
      "        33.0       0.13      0.10      0.11       184\n",
      "        34.0       0.18      0.20      0.19       194\n",
      "        35.0       0.18      0.17      0.17       160\n",
      "        36.0       0.17      0.15      0.16       242\n",
      "        37.0       0.15      0.10      0.12       168\n",
      "        38.0       0.09      0.10      0.10       217\n",
      "        39.0       0.14      0.11      0.12       148\n",
      "        40.0       0.11      0.11      0.11       161\n",
      "        41.0       0.14      0.12      0.13       163\n",
      "        42.0       0.21      0.17      0.19       183\n",
      "        43.0       0.07      0.06      0.07       136\n",
      "        44.0       0.16      0.15      0.16       168\n",
      "        45.0       0.07      0.05      0.06       110\n",
      "        46.0       0.12      0.13      0.12       151\n",
      "        47.0       0.03      0.02      0.03       100\n",
      "        48.0       0.09      0.09      0.09       135\n",
      "        49.0       0.08      0.09      0.08        93\n",
      "        50.0       0.10      0.08      0.09       112\n",
      "        51.0       0.04      0.03      0.03        94\n",
      "        52.0       0.12      0.09      0.10        89\n",
      "        53.0       0.08      0.05      0.06        87\n",
      "        54.0       0.11      0.08      0.09        96\n",
      "        55.0       0.15      0.11      0.12        55\n",
      "        56.0       0.09      0.10      0.09        80\n",
      "        57.0       0.06      0.06      0.06        36\n",
      "        58.0       0.05      0.03      0.04        64\n",
      "        59.0       0.03      0.03      0.03        32\n",
      "        60.0       0.00      0.00      0.00        35\n",
      "        61.0       0.09      0.07      0.08        45\n",
      "        62.0       0.08      0.04      0.05        54\n",
      "        63.0       0.11      0.05      0.07        38\n",
      "        64.0       0.15      0.13      0.14        31\n",
      "        65.0       0.00      0.00      0.00        14\n",
      "        66.0       0.25      0.11      0.15        38\n",
      "        67.0       0.00      0.00      0.00        17\n",
      "        68.0       0.10      0.08      0.09        13\n",
      "        69.0       0.00      0.00      0.00        13\n",
      "        70.0       0.00      0.00      0.00        13\n",
      "        71.0       0.00      0.00      0.00        12\n",
      "        72.0       0.00      0.00      0.00        10\n",
      "        73.0       0.00      0.00      0.00         5\n",
      "        74.0       0.00      0.00      0.00        10\n",
      "        75.0       0.00      0.00      0.00         2\n",
      "        76.0       0.00      0.00      0.00         1\n",
      "        77.0       0.00      0.00      0.00         8\n",
      "        78.0       0.00      0.00      0.00         1\n",
      "        79.0       0.00      0.00      0.00         2\n",
      "        80.0       0.00      0.00      0.00         1\n",
      "        81.0       0.00      0.00      0.00         1\n",
      "        82.0       0.00      0.00      0.00         0\n",
      "        84.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.23      9214\n",
      "   macro avg       0.15      0.17      0.15      9214\n",
      "weighted avg       0.21      0.23      0.21      9214\n",
      "\n",
      "Shape of X_cat: (46068, 34)\n",
      "Shape of combined_features: (46068, 1100)\n",
      "Type of X_cat: <class 'pandas.core.frame.DataFrame'>\n",
      "Type of combined_features: <class 'scipy.sparse._coo.coo_matrix'>\n",
      "Dtype of X_cat: cinsiyet                                                float64\n",
      "dogum_yeri                                               object\n",
      "ikametgah_sehri                                          object\n",
      "universite_adi                                           object\n",
      "universite_turu                                          object\n",
      "burslu_ise_burs_yuzdesi                                 float64\n",
      "burs_aliyor_mu?                                           int64\n",
      "universite_kacinci_sinif                                 object\n",
      "universite_not_ortalamasi                               float64\n",
      "daha_once_baska_bir_universiteden_mezun_olmus             int64\n",
      "lise_mezuniyet_notu                                      object\n",
      "baska_bir_kurumdan_burs_aliyor_mu?                      float64\n",
      "baska_kurumdan_aldigi_burs_miktari                       object\n",
      "anne_calisma_durumu                                       int64\n",
      "baba_calisma_durumu                                       int64\n",
      "kardes_sayisi                                           float64\n",
      "girisimcilik_kulupleri_tarzi_bir_kulube_uye_misiniz?    float64\n",
      "profesyonel_bir_spor_daliyla_mesgul_musunuz?            float64\n",
      "aktif_olarak_bir_stk_uyesi_misiniz?                       int64\n",
      "stk_projesine_katildiniz_mi?                              int64\n",
      "girisimcilikle_ilgili_deneyiminiz_var_mi?                object\n",
      "ingilizce_biliyor_musunuz?                                int64\n",
      "daha_onceden_mezun_olunduysa_mezun_olunan_universite     object\n",
      "anne_sektor_encoded                                       int64\n",
      "baba_sektor_encoded                                       int64\n",
      "anne_unknown                                               bool\n",
      "anne_diger                                                 bool\n",
      "anne_kamu                                                  bool\n",
      "anne_ozel_sektor                                           bool\n",
      "baba_unknown                                               bool\n",
      "baba_diger                                                 bool\n",
      "baba_kamu                                                  bool\n",
      "baba_ozel_sektor                                           bool\n",
      "age                                                       int64\n",
      "dtype: object\n",
      "Dtype of combined_features: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mustafa\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Mustafa\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Mustafa\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Mustafa\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Mustafa\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Mustafa\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_cat_preprocessed: (46068, 1923)\n",
      "Shape of combined_features: (46068, 1100)\n",
      "Shape of X_combined: (46068, 3023)\n",
      "Learning rate set to 0.487525\n",
      "0:\tlearn: 10.4264241\ttotal: 292ms\tremaining: 28.9s\n",
      "1:\tlearn: 8.5224544\ttotal: 378ms\tremaining: 18.5s\n",
      "2:\tlearn: 7.7337382\ttotal: 470ms\tremaining: 15.2s\n",
      "3:\tlearn: 7.3186812\ttotal: 556ms\tremaining: 13.3s\n",
      "4:\tlearn: 7.0407951\ttotal: 639ms\tremaining: 12.1s\n",
      "5:\tlearn: 6.8816031\ttotal: 727ms\tremaining: 11.4s\n",
      "6:\tlearn: 6.7460097\ttotal: 818ms\tremaining: 10.9s\n",
      "7:\tlearn: 6.6744783\ttotal: 903ms\tremaining: 10.4s\n",
      "8:\tlearn: 6.6245713\ttotal: 982ms\tremaining: 9.93s\n",
      "9:\tlearn: 6.5554497\ttotal: 1.06s\tremaining: 9.55s\n",
      "10:\tlearn: 6.5145613\ttotal: 1.14s\tremaining: 9.26s\n",
      "11:\tlearn: 6.4912274\ttotal: 1.22s\tremaining: 8.92s\n",
      "12:\tlearn: 6.4594504\ttotal: 1.31s\tremaining: 8.79s\n",
      "13:\tlearn: 6.4395715\ttotal: 1.39s\tremaining: 8.54s\n",
      "14:\tlearn: 6.4054485\ttotal: 1.48s\tremaining: 8.4s\n",
      "15:\tlearn: 6.3914201\ttotal: 1.57s\tremaining: 8.22s\n",
      "16:\tlearn: 6.3762029\ttotal: 1.64s\tremaining: 8.01s\n",
      "17:\tlearn: 6.3520002\ttotal: 1.73s\tremaining: 7.86s\n",
      "18:\tlearn: 6.3395780\ttotal: 1.81s\tremaining: 7.7s\n",
      "19:\tlearn: 6.3237467\ttotal: 1.88s\tremaining: 7.53s\n",
      "20:\tlearn: 6.2948742\ttotal: 1.97s\tremaining: 7.42s\n",
      "21:\tlearn: 6.2419990\ttotal: 2.06s\tremaining: 7.29s\n",
      "22:\tlearn: 6.2337546\ttotal: 2.14s\tremaining: 7.16s\n",
      "23:\tlearn: 6.2187940\ttotal: 2.22s\tremaining: 7.04s\n",
      "24:\tlearn: 6.2067696\ttotal: 2.3s\tremaining: 6.91s\n",
      "25:\tlearn: 6.1964583\ttotal: 2.38s\tremaining: 6.79s\n",
      "26:\tlearn: 6.1832584\ttotal: 2.47s\tremaining: 6.67s\n",
      "27:\tlearn: 6.1594767\ttotal: 2.55s\tremaining: 6.56s\n",
      "28:\tlearn: 6.1511683\ttotal: 2.63s\tremaining: 6.44s\n",
      "29:\tlearn: 6.1490581\ttotal: 2.7s\tremaining: 6.3s\n",
      "30:\tlearn: 6.1410646\ttotal: 2.78s\tremaining: 6.18s\n",
      "31:\tlearn: 6.1314959\ttotal: 2.86s\tremaining: 6.08s\n",
      "32:\tlearn: 6.1286708\ttotal: 2.93s\tremaining: 5.95s\n",
      "33:\tlearn: 6.1196603\ttotal: 3.01s\tremaining: 5.84s\n",
      "34:\tlearn: 6.1059003\ttotal: 3.09s\tremaining: 5.74s\n",
      "35:\tlearn: 6.0979041\ttotal: 3.16s\tremaining: 5.62s\n",
      "36:\tlearn: 6.0869756\ttotal: 3.25s\tremaining: 5.53s\n",
      "37:\tlearn: 6.0790122\ttotal: 3.33s\tremaining: 5.43s\n",
      "38:\tlearn: 6.0724922\ttotal: 3.42s\tremaining: 5.34s\n",
      "39:\tlearn: 6.0711517\ttotal: 3.49s\tremaining: 5.23s\n",
      "40:\tlearn: 6.0636376\ttotal: 3.57s\tremaining: 5.14s\n",
      "41:\tlearn: 6.0481543\ttotal: 3.65s\tremaining: 5.04s\n",
      "42:\tlearn: 6.0385741\ttotal: 3.74s\tremaining: 4.96s\n",
      "43:\tlearn: 6.0325514\ttotal: 3.81s\tremaining: 4.85s\n",
      "44:\tlearn: 6.0257784\ttotal: 3.89s\tremaining: 4.76s\n",
      "45:\tlearn: 6.0186587\ttotal: 3.98s\tremaining: 4.67s\n",
      "46:\tlearn: 6.0119432\ttotal: 4.06s\tremaining: 4.58s\n",
      "47:\tlearn: 6.0041652\ttotal: 4.13s\tremaining: 4.47s\n",
      "48:\tlearn: 5.9975028\ttotal: 4.21s\tremaining: 4.38s\n",
      "49:\tlearn: 5.9918348\ttotal: 4.29s\tremaining: 4.29s\n",
      "50:\tlearn: 5.9854624\ttotal: 4.37s\tremaining: 4.2s\n",
      "51:\tlearn: 5.9786013\ttotal: 4.45s\tremaining: 4.11s\n",
      "52:\tlearn: 5.9616493\ttotal: 4.53s\tremaining: 4.02s\n",
      "53:\tlearn: 5.9484529\ttotal: 4.61s\tremaining: 3.93s\n",
      "54:\tlearn: 5.9434413\ttotal: 4.68s\tremaining: 3.83s\n",
      "55:\tlearn: 5.9354099\ttotal: 4.76s\tremaining: 3.74s\n",
      "56:\tlearn: 5.9221289\ttotal: 4.85s\tremaining: 3.65s\n",
      "57:\tlearn: 5.9132268\ttotal: 4.93s\tremaining: 3.57s\n",
      "58:\tlearn: 5.9060491\ttotal: 5s\tremaining: 3.48s\n",
      "59:\tlearn: 5.8998664\ttotal: 5.08s\tremaining: 3.38s\n",
      "60:\tlearn: 5.8944154\ttotal: 5.16s\tremaining: 3.3s\n",
      "61:\tlearn: 5.8884217\ttotal: 5.24s\tremaining: 3.21s\n",
      "62:\tlearn: 5.8862577\ttotal: 5.32s\tremaining: 3.12s\n",
      "63:\tlearn: 5.8827356\ttotal: 5.39s\tremaining: 3.03s\n",
      "64:\tlearn: 5.8766186\ttotal: 5.47s\tremaining: 2.95s\n",
      "65:\tlearn: 5.8699747\ttotal: 5.55s\tremaining: 2.86s\n",
      "66:\tlearn: 5.8655154\ttotal: 5.63s\tremaining: 2.77s\n",
      "67:\tlearn: 5.8644635\ttotal: 5.69s\tremaining: 2.68s\n",
      "68:\tlearn: 5.8598676\ttotal: 5.77s\tremaining: 2.59s\n",
      "69:\tlearn: 5.8516544\ttotal: 5.85s\tremaining: 2.51s\n",
      "70:\tlearn: 5.8457748\ttotal: 5.92s\tremaining: 2.42s\n",
      "71:\tlearn: 5.8402668\ttotal: 6s\tremaining: 2.33s\n",
      "72:\tlearn: 5.8365405\ttotal: 6.08s\tremaining: 2.25s\n",
      "73:\tlearn: 5.8280937\ttotal: 6.16s\tremaining: 2.16s\n",
      "74:\tlearn: 5.8219335\ttotal: 6.23s\tremaining: 2.08s\n",
      "75:\tlearn: 5.8162609\ttotal: 6.31s\tremaining: 1.99s\n",
      "76:\tlearn: 5.8124406\ttotal: 6.39s\tremaining: 1.91s\n",
      "77:\tlearn: 5.8046939\ttotal: 6.46s\tremaining: 1.82s\n",
      "78:\tlearn: 5.8030712\ttotal: 6.53s\tremaining: 1.74s\n",
      "79:\tlearn: 5.7983339\ttotal: 6.61s\tremaining: 1.65s\n",
      "80:\tlearn: 5.7973885\ttotal: 6.69s\tremaining: 1.57s\n",
      "81:\tlearn: 5.7881039\ttotal: 6.77s\tremaining: 1.49s\n",
      "82:\tlearn: 5.7828366\ttotal: 6.84s\tremaining: 1.4s\n",
      "83:\tlearn: 5.7757416\ttotal: 6.92s\tremaining: 1.32s\n",
      "84:\tlearn: 5.7692886\ttotal: 6.99s\tremaining: 1.23s\n",
      "85:\tlearn: 5.7634996\ttotal: 7.07s\tremaining: 1.15s\n",
      "86:\tlearn: 5.7617639\ttotal: 7.15s\tremaining: 1.07s\n",
      "87:\tlearn: 5.7543324\ttotal: 7.23s\tremaining: 986ms\n",
      "88:\tlearn: 5.7498506\ttotal: 7.31s\tremaining: 903ms\n",
      "89:\tlearn: 5.7448808\ttotal: 7.39s\tremaining: 821ms\n",
      "90:\tlearn: 5.7387338\ttotal: 7.47s\tremaining: 739ms\n",
      "91:\tlearn: 5.7339084\ttotal: 7.54s\tremaining: 656ms\n",
      "92:\tlearn: 5.7295412\ttotal: 7.62s\tremaining: 574ms\n",
      "93:\tlearn: 5.7230990\ttotal: 7.7s\tremaining: 491ms\n",
      "94:\tlearn: 5.7089936\ttotal: 7.78s\tremaining: 410ms\n",
      "95:\tlearn: 5.7038632\ttotal: 7.85s\tremaining: 327ms\n",
      "96:\tlearn: 5.6991658\ttotal: 7.93s\tremaining: 245ms\n",
      "97:\tlearn: 5.6979170\ttotal: 8.01s\tremaining: 163ms\n",
      "98:\tlearn: 5.6970678\ttotal: 8.08s\tremaining: 81.6ms\n",
      "99:\tlearn: 5.6929462\ttotal: 8.15s\tremaining: 0us\n",
      "0:\tlearn: 15.1637351\ttest: 15.2161466\tbest: 15.2161466 (0)\ttotal: 334ms\tremaining: 11m 8s\n",
      "100:\tlearn: 8.8927413\ttest: 8.9958816\tbest: 8.9958816 (100)\ttotal: 25.7s\tremaining: 8m 3s\n",
      "200:\tlearn: 7.2354916\ttest: 7.3834649\tbest: 7.3834649 (200)\ttotal: 48.7s\tremaining: 7m 16s\n",
      "300:\tlearn: 6.6881146\ttest: 6.8594075\tbest: 6.8594075 (300)\ttotal: 1m 11s\tremaining: 6m 41s\n",
      "400:\tlearn: 6.4420297\ttest: 6.6313060\tbest: 6.6313060 (400)\ttotal: 1m 39s\tremaining: 6m 35s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, mean_squared_error\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('cleanedData.csv')\n",
    "\n",
    "# Drop rows with missing target values\n",
    "df = df.dropna(subset=['degerlendirme_puani'])\n",
    "\n",
    "# Extract relevant columns for the classification model\n",
    "cat_features = [\n",
    "    'cinsiyet', 'dogum_yeri', 'ikametgah_sehri', 'universite_adi', 'universite_turu',\n",
    "    'burslu_ise_burs_yuzdesi', 'burs_aliyor_mu?', 'universite_kacinci_sinif',\n",
    "    'universite_not_ortalamasi', 'daha_once_baska_bir_universiteden_mezun_olmus',\n",
    "     'lise_mezuniyet_notu', 'baska_bir_kurumdan_burs_aliyor_mu?',\n",
    "    'baska_kurumdan_aldigi_burs_miktari', \n",
    "    'anne_calisma_durumu', 'baba_calisma_durumu', 'kardes_sayisi',\n",
    "    'girisimcilik_kulupleri_tarzi_bir_kulube_uye_misiniz?',\n",
    "    'profesyonel_bir_spor_daliyla_mesgul_musunuz?',\n",
    "    'aktif_olarak_bir_stk_uyesi_misiniz?',\n",
    "    'stk_projesine_katildiniz_mi?', 'girisimcilikle_ilgili_deneyiminiz_var_mi?',\n",
    "    'ingilizce_biliyor_musunuz?', \n",
    "    'daha_onceden_mezun_olunduysa_mezun_olunan_universite', 'anne_sektor_encoded',\n",
    "    'baba_sektor_encoded', 'anne_unknown', 'anne_diger', 'anne_kamu',\n",
    "    'anne_ozel_sektor', 'baba_unknown', 'baba_diger', 'baba_kamu', 'baba_ozel_sektor',\n",
    "    'age'\n",
    "]\n",
    "\n",
    "text_columns = ['girisimcilikle_ilgili_deneyiminizi_aciklayabilir_misiniz?',\"bolum\",\"lise_adi\",'lise_adi_diger', 'lise_sehir', 'lise_turu', 'lise_bolumu',\n",
    "    'lise_bolum_diger', 'uye_oldugunuz_kulubun_ismi', 'burs_aldigi_baska_kurum','anne_egitim_durumu', 'baba_egitim_durumu', 'spor_dalindaki_rolunuz_nedir?', \"hangi_stk_nin_uyesisiniz?\", 'ingilizce_seviyeniz?']\n",
    "target_column = 'degerlendirme_puani'\n",
    "\n",
    "# Handle missing values in text columns\n",
    "df[text_columns] = df[text_columns].fillna('')\n",
    "\n",
    "# Combine text columns into one for processing\n",
    "df['combined_text'] = df[text_columns].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# Tokenize and preprocess text data for Word2Vec\n",
    "def preprocess_text(text):\n",
    "    return simple_preprocess(text, deacc=True)\n",
    "\n",
    "# Check if the Word2Vec model already exists\n",
    "import os\n",
    "if os.path.exists('word2vec_model.model'):\n",
    "    # Load the previously saved Word2Vec model\n",
    "    word2vec_model = Word2Vec.load('word2vec_model.model')\n",
    "else:\n",
    "    # Prepare Word2Vec model\n",
    "    sentences = [preprocess_text(text) for text in df['combined_text']]\n",
    "    word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    # Save the Word2Vec model\n",
    "    word2vec_model.save('word2vec_model.model')\n",
    "\n",
    "# Create document vectors by averaging word vectors\n",
    "def vectorize_text(text):\n",
    "    tokens = preprocess_text(text)\n",
    "    vectors = [word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "df['text_vector'] = df['combined_text'].apply(vectorize_text)\n",
    "word2vec_features = np.array(df['text_vector'].tolist())\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "text_data = df['combined_text']\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Combine Word2Vec and TF-IDF features\n",
    "combined_features = hstack([tfidf_vectors, word2vec_features])\n",
    "\n",
    "# Debugging step: Verify columns in df and missing columns in cat_features\n",
    "missing_cols = [col for col in cat_features if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(\"Warning: The following columns are missing from the DataFrame:\", missing_cols)\n",
    "\n",
    "# Prepare data for classification model\n",
    "try:\n",
    "    X_cat = df[cat_features]\n",
    "except KeyError as e:\n",
    "    print(\"Error: One or more columns are missing:\", e)\n",
    "    print(\"Available columns:\", df.columns)\n",
    "    raise\n",
    "\n",
    "y = df[target_column]\n",
    "\n",
    "# ColumnTransformer for encoding categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Classification pipeline\n",
    "clf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Split data for classification model\n",
    "X_cat_train, X_cat_test, y_train, y_test = train_test_split(X_cat, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train classification model\n",
    "clf_pipeline.fit(X_cat_train, y_train)\n",
    "\n",
    "# Evaluate classification model\n",
    "y_pred_cat = clf_pipeline.predict(X_cat_test)\n",
    "print(\"Classification Model Accuracy:\", accuracy_score(y_test, y_pred_cat))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_cat))\n",
    "\n",
    "# Prepare data for regression model\n",
    "# Debugging step: Print shapes and types\n",
    "print(\"Shape of X_cat:\", X_cat.shape)\n",
    "print(\"Shape of combined_features:\", combined_features.shape)\n",
    "print(\"Type of X_cat:\", type(X_cat))\n",
    "print(\"Type of combined_features:\", type(combined_features))\n",
    "print(\"Dtype of X_cat:\", X_cat.dtypes)\n",
    "print(\"Dtype of combined_features:\", combined_features.dtype)\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = X_cat.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X_cat.select_dtypes(include=['object', 'bool']).columns\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=True), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Fit and transform the data\n",
    "X_cat_preprocessed = preprocessor.fit_transform(X_cat)\n",
    "\n",
    "# Debugging step: Print shapes after preprocessing\n",
    "print(\"Shape of X_cat_preprocessed:\", X_cat_preprocessed.shape)\n",
    "print(\"Shape of combined_features:\", combined_features.shape)\n",
    "\n",
    "# Combine features\n",
    "X_combined = hstack([X_cat_preprocessed, combined_features])\n",
    "\n",
    "# Debugging step: Print final combined shape\n",
    "print(\"Shape of X_combined:\", X_combined.shape)\n",
    "\n",
    "# Split data for regression model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Modify CatBoost hyperparameters\n",
    "catboost_model = CatBoostRegressor(\n",
    "    iterations=2000,  # Increased iterations\n",
    "    learning_rate=0.01,  # Lowered learning rate\n",
    "    depth=8,  # Increased depth\n",
    "    l2_leaf_reg=3,  # Added L2 regularization\n",
    "    loss_function='RMSE',\n",
    "    eval_metric='RMSE',\n",
    "    random_seed=42,\n",
    "    verbose=100,\n",
    "    early_stopping_rounds=500  # Increased early stopping rounds\n",
    ")\n",
    "\n",
    "# Add feature selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "selector = SelectFromModel(estimator=CatBoostRegressor(iterations=100), threshold='median')\n",
    "X_selected = selector.fit_transform(X_combined, y)\n",
    "\n",
    "# Split data using selected features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Train the model with selected features\n",
    "catboost_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=(X_test, y_test),\n",
    "    use_best_model=True,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(catboost_model, X_combined, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f'Cross-validation RMSE: {(-cv_scores.mean())**0.5:.4f} (+/- {cv_scores.std() * 2:.4f})')\n",
    "\n",
    "# Predict and evaluate regression model\n",
    "y_pred_reg = catboost_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred_reg)\n",
    "rmse = mse ** 0.5\n",
    "print(f'Regression Model RMSE: {rmse:.4f}')\n",
    "\n",
    "# Print actual vs predicted values for regression\n",
    "print(\"\\nSample Actual vs. Predicted Values:\")\n",
    "for actual, predicted in zip(y_test[:10], y_pred_reg[:10]):\n",
    "    print(f\"Actual: {actual:.2f}, Predicted: {predicted:.2f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = catboost_model.feature_importances_\n",
    "feature_names = preprocessor.get_feature_names_out().tolist() + [f'text_feature_{i}' for i in range(combined_features.shape[1])]\n",
    "for name, importance in sorted(zip(feature_names, feature_importance), key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print(f\"Feature: {name}, Importance: {importance:.4f}\")\n",
    "\n",
    "# Save the CatBoost model\n",
    "catboost_model.save_model('catboost_model.cbm')\n",
    "\n",
    "# Load the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63b1a4d-5df7-4f95-a140-b0c96c72218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('cleanTest.csv')\n",
    "\n",
    "# Preprocess test data\n",
    "test_df[text_columns] = test_df[text_columns].fillna('')\n",
    "test_df['combined_text'] = test_df[text_columns].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# Create document vectors for test data\n",
    "test_df['text_vector'] = test_df['combined_text'].apply(vectorize_text)\n",
    "test_word2vec_features = np.array(test_df['text_vector'].tolist())\n",
    "\n",
    "# TF-IDF Vectorization for test data\n",
    "test_tfidf_vectors = tfidf_vectorizer.transform(test_df['combined_text'])\n",
    "\n",
    "# Combine Word2Vec and TF-IDF features for test data\n",
    "test_combined_features = hstack([test_tfidf_vectors, test_word2vec_features])\n",
    "\n",
    "# Prepare test data for the model\n",
    "X_test_cat = test_df[cat_features]\n",
    "\n",
    "# Preprocess test data\n",
    "X_test_cat_preprocessed = preprocessor.transform(X_test_cat)\n",
    "\n",
    "# Combine features for test data\n",
    "X_test_combined = hstack([X_test_cat_preprocessed, test_combined_features])\n",
    "\n",
    "# Apply feature selection\n",
    "X_test_selected = selector.transform(X_test_combined)\n",
    "\n",
    "# Load the saved CatBoost model\n",
    "loaded_model = CatBoostRegressor()\n",
    "loaded_model.load_model('catboost_model.cbm')\n",
    "\n",
    "# Make predictions\n",
    "test_predictions = loaded_model.predict(X_test_selected)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'Degerlendirme Puani': test_predictions\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Predictions have been saved to 'submission.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ff4f1a-4090-4a28-8a83-8d72038276e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
